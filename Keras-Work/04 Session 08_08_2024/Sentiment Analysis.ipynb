{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS-j-NEMM_zi",
        "outputId": "aa5befda-91b1-4847-ce37-1762532920a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/572.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m572.2/572.2 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/5.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m5.1/5.2 MB\u001b[0m \u001b[31m152.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade keras-nlp\n",
        "!pip install -q --upgrade keras  # Upgrade to Keras 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HQHttRI2NHYn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
        "\n",
        "import keras_nlp\n",
        "import keras\n",
        "\n",
        "# Use mixed precision to speed up all training in this guide.\n",
        "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccGfHva6NhII",
        "outputId": "17ed36ab-d766-46a2-e43f-baa11db1f80f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  2039k      0  0:00:40  0:00:40 --:--:-- 1599k\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz\n",
        "!# Remove unsupervised examples\n",
        "!rm -r aclImdb/train/unsup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udQONZRQNwnY",
        "outputId": "a3e54b31-bb04-4787-f14e-4725dd06d806"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 25000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "(<tf.Tensor: shape=(), dtype=string, numpy=b'It is rare that one comes across a movie as flawless as this. It\\'s truly one of the best acted, most tightly structured films I\\'ve ever seen. Every line of dialogue can be interpreted in several ways, relating to each of the three main characters differently. The film weaves an intrinsic web of motivations and double crosses that snare you and refuse to let go. Add to this that the slow-burning romance between Kevin and Faye is as moving as anything that\\'s ever been committed to celluloid and you have the ingredients for a perfect film. It exposes the romance of movies such as \"Titanic\" as the trite cliches they are. If you\\'re looking for a movie to watch while you fold laundry, this isn\\'t it. You have to commit yourself to this film. You can\\'t have a conversation while running in and out of the room. This movie demands your attention. Treat it with the respect you deserve and you\\'ll get a lot out of it. Unless you think \"Titanic\" is the greatest film ever.'>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 16\n",
        "imdb_train = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "imdb_test = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\",\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "\n",
        "# Inspect first review\n",
        "# Format is (review text tensor, label tensor)\n",
        "print(imdb_train.unbatch().take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ej6leRsmPYtt",
        "outputId": "481135b8-1600-4a96-c817-cb332d5ced60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased_sst2/4/download/metadata.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 145/145 [00:00<00:00, 409kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased_sst2/4/download/task.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2.04k/2.04k [00:00<00:00, 5.15MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased_sst2/4/download/task.weights.h5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50.3M/50.3M [00:05<00:00, 10.5MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'loss_scale_optimizer', because it has 4 variables whereas the saved optimizer has 84 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 0 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased_sst2/4/download/model.weights.h5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16.8M/16.8M [00:02<00:00, 6.68MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased_sst2/4/download/assets/tokenizer/vocabulary.txt...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 226k/226k [00:00<00:00, 274kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 918ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[-1.289,  1.395]], dtype=float16)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\"bert_tiny_en_uncased_sst2\")\n",
        "# Note: batched inputs expected so must wrap string in iterable\n",
        "classifier.predict([\"I love modular workflows in keras-nlp!\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4OHCDd6Pb6W",
        "outputId": "2ecf9db9-3f40-4ba2-9501-bc197945e86f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - loss: 0.4553 - sparse_categorical_accuracy: 0.7908\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.4550047814846039, 0.7902799844741821]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.evaluate(imdb_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fu7u7J1kPeuQ",
        "outputId": "98a16927-bf95-47ae-81e0-ff4930dcd568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/metadata.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 139/139 [00:00<00:00, 164kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/config.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 507/507 [00:00<00:00, 592kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/model.weights.h5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 16.8M/16.8M [00:02<00:00, 6.45MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/tokenizer.json...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 547/547 [00:00<00:00, 209kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/models/keras/bert/keras/bert_tiny_en_uncased/2/download/assets/tokenizer/vocabulary.txt...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 226k/226k [00:00<00:00, 284kB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 36ms/step - loss: 0.5130 - sparse_categorical_accuracy: 0.7344 - val_loss: 0.3055 - val_sparse_categorical_accuracy: 0.8709\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78e6f050f610>"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    num_classes=2,\n",
        ")\n",
        "classifier.fit(\n",
        "    imdb_train,\n",
        "    validation_data=imdb_test,\n",
        "    epochs=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7oUYd9XQHR3",
        "outputId": "d79a4e38-6bbc-4626-d413-da66cce8569e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 33ms/step - loss: 0.5175 - sparse_categorical_accuracy: 0.7249 - val_loss: 0.2995 - val_sparse_categorical_accuracy: 0.8743\n",
            "Epoch 2/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 19ms/step - loss: 0.2728 - sparse_categorical_accuracy: 0.8914 - val_loss: 0.2871 - val_sparse_categorical_accuracy: 0.8832\n",
            "Epoch 3/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 31ms/step - loss: 0.2009 - sparse_categorical_accuracy: 0.9220 - val_loss: 0.3101 - val_sparse_categorical_accuracy: 0.8799\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78e68c133100>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\n",
        "    \"bert_tiny_en_uncased\",\n",
        "    sequence_length=512,\n",
        ")\n",
        "\n",
        "# Apply the preprocessor to every sample of train and test data using `map()`.\n",
        "# [`tf.data.AUTOTUNE`](https://www.tensorflow.org/api_docs/python/tf/data/AUTOTUNE) and `prefetch()` are options to tune performance, see\n",
        "# https://www.tensorflow.org/guide/data_performance for details.\n",
        "\n",
        "# Note: only call `cache()` if you training data fits in CPU memory!\n",
        "imdb_train_cached = (\n",
        "    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "imdb_test_cached = (\n",
        "    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
        "    \"bert_tiny_en_uncased\", preprocessor=None, num_classes=2\n",
        ")\n",
        "classifier.fit(\n",
        "    imdb_train_cached,\n",
        "    validation_data=imdb_test_cached,\n",
        "    epochs=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmMXxO75Q0_l",
        "outputId": "7f00ddeb-e368-4024-be74-c2796798b537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "({'token_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
            "array([  101,  2307,  1039,  5856,  3896,  1004,  1037,  5621,  7436,\n",
            "        1011, 11007,  2836,  2011,  5639,  8254,  5562,  2004,  8318,\n",
            "        1012,  4907,  1012,  1026,  7987,  1013,  1028,  1026,  7987,\n",
            "        1013,  1028,  3419,  9180,  2015,  2003,  1037,  2028,  1011,\n",
            "        7577, 15606,  1999,  2023,  3185,  1010,  2129,  2002,  2288,\n",
            "        1996,  2190,  3364,  7436,  2008,  2095,  2058,  5253, 11462,\n",
            "        2001,  1037,  4126,  1012,  1026,  7987,  1013,  1028,  1026,\n",
            "         102], dtype=int32)>, 'segment_ids': <tf.Tensor: shape=(64,), dtype=int32, numpy=\n",
            "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "      dtype=int32)>, 'padding_mask': <tf.Tensor: shape=(64,), dtype=bool, numpy=\n",
            "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "        True])>}, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
          ]
        }
      ],
      "source": [
        "tokenizer = keras_nlp.models.BertTokenizer.from_preset(\"bert_tiny_en_uncased\")\n",
        "tokenizer([\"I love modular workflows!\", \"Libraries over frameworks!\"])\n",
        "\n",
        "# Write your own packer or use one of our `Layers`\n",
        "packer = keras_nlp.layers.MultiSegmentPacker(\n",
        "    start_value=tokenizer.cls_token_id,\n",
        "    end_value=tokenizer.sep_token_id,\n",
        "    # Note: This cannot be longer than the preset's `sequence_length`, and there\n",
        "    # is no check for a custom preprocessor!\n",
        "    sequence_length=64,\n",
        ")\n",
        "\n",
        "\n",
        "# This function that takes a text sample `x` and its\n",
        "# corresponding label `y` as input and converts the\n",
        "# text into a format suitable for input into a BERT model.\n",
        "def preprocessor(x, y):\n",
        "    token_ids, segment_ids = packer(tokenizer(x))\n",
        "    x = {\n",
        "        \"token_ids\": token_ids,\n",
        "        \"segment_ids\": segment_ids,\n",
        "        \"padding_mask\": token_ids != 0,\n",
        "    }\n",
        "    return x, y\n",
        "\n",
        "\n",
        "imdb_train_preprocessed = imdb_train.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")\n",
        "imdb_test_preprocessed = imdb_test.map(preprocessor, tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")\n",
        "\n",
        "# Preprocessed example\n",
        "print(imdb_train_preprocessed.unbatch().take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "id": "5EAy7FNmQyMe",
        "outputId": "34f8c2e0-9a7e-4c08-c242-a74f8dda238e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ segment_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bert_backbone             │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,   │      <span style=\"color: #00af00; text-decoration-color: #00af00\">4,385,920</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertBackbone</span>)            │ <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)]            │                │ segment_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],     │\n",
              "│                           │                        │                │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ bert_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">198,272</span> │ transformer_encoder[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ transformer_encoder_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │ get_item_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ segment_ids (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ bert_backbone             │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,   │      \u001b[38;5;34m4,385,920\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
              "│ (\u001b[38;5;33mBertBackbone\u001b[0m)            │ \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)]            │                │ segment_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],     │\n",
              "│                           │                        │                │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m198,272\u001b[0m │ bert_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ transformer_encoder_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m198,272\u001b[0m │ transformer_encoder[\u001b[38;5;34m0\u001b[0m… │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)      │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ get_item_4 (\u001b[38;5;33mGetItem\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ transformer_encoder_1… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m258\u001b[0m │ get_item_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,782,722</span> (18.24 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,782,722\u001b[0m (18.24 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">396,802</span> (1.51 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m396,802\u001b[0m (1.51 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,385,920</span> (16.73 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,385,920\u001b[0m (16.73 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 38ms/step - loss: 0.6912 - sparse_categorical_accuracy: 0.6406 - val_loss: 0.5161 - val_sparse_categorical_accuracy: 0.7470\n",
            "Epoch 2/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 24ms/step - loss: 0.5378 - sparse_categorical_accuracy: 0.7294 - val_loss: 0.5012 - val_sparse_categorical_accuracy: 0.7559\n",
            "Epoch 3/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 23ms/step - loss: 0.5117 - sparse_categorical_accuracy: 0.7464 - val_loss: 0.4470 - val_sparse_categorical_accuracy: 0.7923\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78e6f010e650>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preprocessor = keras_nlp.models.BertPreprocessor.from_preset(\"bert_tiny_en_uncased\")\n",
        "backbone = keras_nlp.models.BertBackbone.from_preset(\"bert_tiny_en_uncased\")\n",
        "\n",
        "imdb_train_preprocessed = (\n",
        "    imdb_train.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "imdb_test_preprocessed = (\n",
        "    imdb_test.map(preprocessor, tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "backbone.trainable = False\n",
        "inputs = backbone.input\n",
        "sequence = backbone(inputs)[\"sequence_output\"]\n",
        "for _ in range(2):\n",
        "    sequence = keras_nlp.layers.TransformerEncoder(\n",
        "        num_heads=2,\n",
        "        intermediate_dim=512,\n",
        "        dropout=0.1,\n",
        "    )(sequence)\n",
        "# Use [CLS] token output to classify\n",
        "outputs = keras.layers.Dense(2)(sequence[:, backbone.cls_token_index, :])\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.AdamW(5e-5),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    jit_compile=True,\n",
        ")\n",
        "model.summary()\n",
        "model.fit(\n",
        "    imdb_train_preprocessed,\n",
        "    validation_data=imdb_test_preprocessed,\n",
        "    epochs=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6ckD8VoVRszQ"
      },
      "outputs": [],
      "source": [
        "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "    imdb_train.map(lambda x, y: x),\n",
        "    vocabulary_size=20_000,\n",
        "    lowercase=True,\n",
        "    strip_accents=True,\n",
        "    reserved_tokens=[\"[PAD]\", \"[START]\", \"[END]\", \"[MASK]\", \"[UNK]\"],\n",
        ")\n",
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    lowercase=True,\n",
        "    strip_accents=True,\n",
        "    oov_token=\"[UNK]\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFrybGVtRuMo",
        "outputId": "fc90807f-4a2c-433e-fe5a-047809939ae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(<tf.Tensor: shape=(512,), dtype=int32, numpy=\n",
            "array([    1,  1128,    33,   104,    20,    35,    33,   104,    20,\n",
            "          35,   887,   100,  1581,   103,    44,   451,  1172,    17,\n",
            "         112,    44,  3854,   424,    17,   100,   315,   271,   780,\n",
            "         444,   103,    44,  1651,    19,   231,    52,   304,   228,\n",
            "         108,   107,   103,   507,   129,  4899,   444,  1183,   235,\n",
            "         176,   451,   887,   100,  1581,  3406,    17,   134,   103,\n",
            "         118,   189,   116,   146,   882,   107,  1172,    19,   349,\n",
            "          17,   105,   103,  4797,   102,   228,   108,   107,   103,\n",
            "         124,   101,    99,   346,   101,    99,   179,   887,   100,\n",
            "        1581,   765,    17,   100,    52,   146,   228,   108,    52,\n",
            "        1138,    17,   114,    99,   288,   101,    99,   649,   103,\n",
            "         108,   125,   887,   100,  1581,   765,   120,   180,    17,\n",
            "         114,   144,   120,   137,   223,   168,   505,    19,   167,\n",
            "          17,   107,   156,  1287,   178,     7,   505,    19,     7,\n",
            "          33,   104,    20,    35,    33,   104,    20,    35,   222,\n",
            "         103,    99,   207,   101,    49,  1989,  3369,   347,  1169,\n",
            "          19,   887,   103,  2834,   102,   129,   248,   719,  2014,\n",
            "        4166,    17,   100,   562,   803,  5632,    19,  1581, 15313,\n",
            "         182,  1899,   119,   103,    17,   100,   597,   102,  2489,\n",
            "         183,   154,   147,   168,   119,   560,   103,    19,   119,\n",
            "        2278,   719,  6905,   139,   111,   887,    17,   100,   887,\n",
            "         562,  1899,   140,   101,   121, 10339,    17,   100,   121,\n",
            "         557,    19,  1581,   188,  1583,    44, 10736,   214,    44,\n",
            "       16667, 11122,   100,  8131,   105,    17,  1463,   325,   105,\n",
            "         264,   133,    99, 16667,   103,    44,  1301,    19,   887,\n",
            "         757,   140,   108,  1581,   215,   125,   101,   107,    17,\n",
            "         100,  3472,   183,    19,   887,   188,  2665,  8898,    99,\n",
            "         406,  5330,    19,   287,    17,   126,    99,   225,  1581,\n",
            "         728,   151,    99,   124,  1699,  1899,   196,   119,  1188,\n",
            "         121,  5501,   106,    44,    49,  3160,  2741,  3881,    19,\n",
            "          33,   104,    20,    35,    33,   104,    20,    35,   549,\n",
            "          17,   107,   103,   324,   132,    99,   212,   887,   100,\n",
            "        1581,   444,   140,   134,    17,   114,   107,   103,   226,\n",
            "          44,   348,   100,   547,   518,   101,   152,    19,   105,\n",
            "         263,   166,   749,   108,   134,   109,   147,  2610,   133,\n",
            "         107,    19,    99,  2974,   100,  2153,   106,   107,   108,\n",
            "         190,   887,   100,  1581,   897,   120,   110,   145,   110,\n",
            "         219,   106,   107,   444,    17,   114,   105,   522,   133,\n",
            "         236,   109,  1113,   106,   107,    19,   105,   522,  8190,\n",
            "         168,   185,   887,   100,  1581,  3406,    19,   652,    17,\n",
            "         107,   103,    44,   145,   444,   108,   221,   118,   277,\n",
            "         514,   151,   102,   144,   176,   887,   100,  1581,  2610,\n",
            "          17,   114,   105,   103,   226,   426,    19,    33,   104,\n",
            "          20,    35,    33,   104,    20,    35,    28,    20,   254,\n",
            "           2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "           0,     0,     0,     0,     0,     0,     0,     0],\n",
            "      dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n"
          ]
        }
      ],
      "source": [
        "packer = keras_nlp.layers.StartEndPacker(\n",
        "    start_value=tokenizer.token_to_id(\"[START]\"),\n",
        "    end_value=tokenizer.token_to_id(\"[END]\"),\n",
        "    pad_value=tokenizer.token_to_id(\"[PAD]\"),\n",
        "    sequence_length=512,\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess(x, y):\n",
        "    token_ids = packer(tokenizer(x))\n",
        "    return token_ids, y\n",
        "\n",
        "\n",
        "imdb_preproc_train_ds = imdb_train.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "imdb_preproc_val_ds = imdb_test.map(\n",
        "    preprocess, num_parallel_calls=tf.data.AUTOTUNE\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(imdb_preproc_train_ds.unbatch().take(1).get_single_element())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "QPfrt4wSSCY4",
        "outputId": "281eb526-1b6f-415d-ecac-9a7fd51cef6e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_4\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional_4\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,260,544</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,472</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ get_item_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GetItem</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ token_and_position_embedding         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │       \u001b[38;5;34m1,260,544\u001b[0m │\n",
              "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)          │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ transformer_encoder_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m33,472\u001b[0m │\n",
              "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ get_item_8 (\u001b[38;5;33mGetItem\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                   │             \u001b[38;5;34m130\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,294,146</span> (4.94 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,294,146\u001b[0m (4.94 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,294,146</span> (4.94 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,294,146\u001b[0m (4.94 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "token_id_input = keras.Input(\n",
        "    shape=(None,),\n",
        "    dtype=\"int32\",\n",
        "    name=\"token_ids\",\n",
        ")\n",
        "outputs = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=len(vocab),\n",
        "    sequence_length=packer.sequence_length,\n",
        "    embedding_dim=64,\n",
        ")(token_id_input)\n",
        "outputs = keras_nlp.layers.TransformerEncoder(\n",
        "    num_heads=2,\n",
        "    intermediate_dim=128,\n",
        "    dropout=0.1,\n",
        ")(outputs)\n",
        "# Use \"[START]\" token to classify\n",
        "outputs = keras.layers.Dense(2)(outputs[:, 0, :])\n",
        "model = keras.Model(\n",
        "    inputs=token_id_input,\n",
        "    outputs=outputs,\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPVH7vyASff-",
        "outputId": "82a428e0-8a3b-48b7-f547-9a3e43f7fd06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 18ms/step - loss: 0.7672 - sparse_categorical_accuracy: 0.5242 - val_loss: 0.5860 - val_sparse_categorical_accuracy: 0.6412\n",
            "Epoch 2/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 15ms/step - loss: 0.4369 - sparse_categorical_accuracy: 0.8071 - val_loss: 0.3590 - val_sparse_categorical_accuracy: 0.8439\n",
            "Epoch 3/3\n",
            "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 14ms/step - loss: 0.2700 - sparse_categorical_accuracy: 0.8901 - val_loss: 0.3101 - val_sparse_categorical_accuracy: 0.8715\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x78e3af935840>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.AdamW(5e-5),\n",
        "    metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        "    jit_compile=True,\n",
        ")\n",
        "model.fit(\n",
        "    imdb_preproc_train_ds,\n",
        "    validation_data=imdb_preproc_val_ds,\n",
        "    epochs=3,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
